{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thrones2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â© Yuriy Guts, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the raw text of [A Song of Ice and Fire](https://en.wikipedia.org/wiki/A_Song_of_Ice_and_Fire), we'll derive and explore the semantic properties of its words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word encoding\n",
    "import codecs\n",
    "#finds all pathnames matching a pattern, like regex\n",
    "import glob\n",
    "#log events for libraries\n",
    "import logging\n",
    "#concurrecy\n",
    "import multiprocessing\n",
    "#deal with operation system\n",
    "import os\n",
    "#better print\n",
    "import pprint\n",
    "#regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dev\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tf\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#natural language toolkit\n",
    "import nltk\n",
    "#word 2 vec\n",
    "import gensim.models.word2vec as w2v\n",
    "#dimensionality reduction\n",
    "import sklearn.manifold\n",
    "#math\n",
    "import numpy as np\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "#parse dataset\n",
    "import pandas as pd\n",
    "#visualization\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Download NLTK tokenizer models (only the first time)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load books from files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_filenames = sorted(glob.glob(\"data/*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found books:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data\\\\got1.txt',\n",
       " 'data\\\\got2.txt',\n",
       " 'data\\\\got3.txt',\n",
       " 'data\\\\got4.txt',\n",
       " 'data\\\\got5.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Found books:\")\n",
    "book_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine the books into one string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 'data\\got1.txt'...\n",
      "Corpus is now 1787941 characters long\n",
      "\n",
      "Reading 'data\\got2.txt'...\n",
      "Corpus is now 4110003 characters long\n",
      "\n",
      "Reading 'data\\got3.txt'...\n",
      "Corpus is now 6452402 characters long\n",
      "\n",
      "Reading 'data\\got4.txt'...\n",
      "Corpus is now 8185413 characters long\n",
      "\n",
      "Reading 'data\\got5.txt'...\n",
      "Corpus is now 9811978 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus_raw = u\"\"\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the corpus into sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert into a list of words\n",
    "#rtemove unnnecessary,, split into words, no hyphens\n",
    "#list of words\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentence where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heraldic crest by Virginia Norey.\n",
      "['Heraldic', 'crest', 'by', 'Virginia', 'Norey']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[5])\n",
    "print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 1,818,103 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ONCE we have vectors\n",
    "#step 3 - build model\n",
    "#3 main tasks that vectors help with\n",
    "#DISTANCE, SIMILARITY, RANKING\n",
    "\n",
    "# Dimensionality of the resulting word vectors.\n",
    "#more dimensions, more computationally expensive to train\n",
    "#but also more accurate\n",
    "#more dimensions = more generalized\n",
    "num_features = 300\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "#more workers, faster we train\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thrones2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-12 17:40:37,499 : INFO : collecting all words and their counts\n",
      "2017-06-12 17:40:37,502 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-12 17:40:37,551 : INFO : PROGRESS: at sentence #10000, processed 140984 words, keeping 10280 word types\n",
      "2017-06-12 17:40:37,598 : INFO : PROGRESS: at sentence #20000, processed 279730 words, keeping 13558 word types\n",
      "2017-06-12 17:40:37,653 : INFO : PROGRESS: at sentence #30000, processed 420336 words, keeping 16598 word types\n",
      "2017-06-12 17:40:37,686 : INFO : PROGRESS: at sentence #40000, processed 556581 words, keeping 18324 word types\n",
      "2017-06-12 17:40:37,725 : INFO : PROGRESS: at sentence #50000, processed 686247 words, keeping 19714 word types\n",
      "2017-06-12 17:40:37,770 : INFO : PROGRESS: at sentence #60000, processed 828497 words, keeping 21672 word types\n",
      "2017-06-12 17:40:37,812 : INFO : PROGRESS: at sentence #70000, processed 973830 words, keeping 23093 word types\n",
      "2017-06-12 17:40:37,851 : INFO : PROGRESS: at sentence #80000, processed 1114967 words, keeping 24252 word types\n",
      "2017-06-12 17:40:37,900 : INFO : PROGRESS: at sentence #90000, processed 1260481 words, keeping 26007 word types\n",
      "2017-06-12 17:40:37,933 : INFO : PROGRESS: at sentence #100000, processed 1393203 words, keeping 26884 word types\n",
      "2017-06-12 17:40:37,992 : INFO : PROGRESS: at sentence #110000, processed 1532150 words, keeping 27809 word types\n",
      "2017-06-12 17:40:38,032 : INFO : PROGRESS: at sentence #120000, processed 1680961 words, keeping 28486 word types\n",
      "2017-06-12 17:40:38,071 : INFO : collected 29026 word types from a corpus of 1818103 raw words and 128868 sentences\n",
      "2017-06-12 17:40:38,072 : INFO : Loading a fresh vocabulary\n",
      "2017-06-12 17:40:38,140 : INFO : min_count=3 retains 17277 unique words (59% of original 29026, drops 11749)\n",
      "2017-06-12 17:40:38,143 : INFO : min_count=3 leaves 1802699 word corpus (99% of original 1818103, drops 15404)\n",
      "2017-06-12 17:40:38,213 : INFO : deleting the raw counts dictionary of 29026 items\n",
      "2017-06-12 17:40:38,216 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2017-06-12 17:40:38,217 : INFO : downsampling leaves estimated 1404424 word corpus (77.9% of prior 1802699)\n",
      "2017-06-12 17:40:38,219 : INFO : estimated required memory for 17277 words and 300 dimensions: 50103300 bytes\n",
      "2017-06-12 17:40:38,312 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "thrones2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-50e4ee2aec51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Word2Vec vocabulary length:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthrones2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(thrones2vec.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start training, this might take a minute or two...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-12 17:40:45,541 : INFO : training model with 4 workers on 17277 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=7\n",
      "2017-06-12 17:40:45,544 : INFO : expecting 128868 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-06-12 17:40:46,563 : INFO : PROGRESS: at 2.30% examples, 161287 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:40:47,567 : INFO : PROGRESS: at 4.98% examples, 172754 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:40:48,615 : INFO : PROGRESS: at 7.44% examples, 166633 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:40:49,707 : INFO : PROGRESS: at 10.08% examples, 167450 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:40:50,728 : INFO : PROGRESS: at 12.65% examples, 170326 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:40:51,743 : INFO : PROGRESS: at 15.25% examples, 171019 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:40:52,771 : INFO : PROGRESS: at 17.69% examples, 170074 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:40:53,836 : INFO : PROGRESS: at 20.00% examples, 169520 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:40:54,882 : INFO : PROGRESS: at 22.18% examples, 167072 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:40:55,886 : INFO : PROGRESS: at 24.53% examples, 166507 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:40:56,892 : INFO : PROGRESS: at 27.43% examples, 168802 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:40:57,905 : INFO : PROGRESS: at 30.27% examples, 171216 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:40:58,929 : INFO : PROGRESS: at 32.99% examples, 172564 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:40:59,982 : INFO : PROGRESS: at 36.03% examples, 174324 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:00,984 : INFO : PROGRESS: at 38.17% examples, 172959 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:02,012 : INFO : PROGRESS: at 40.84% examples, 174306 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:41:03,016 : INFO : PROGRESS: at 43.30% examples, 174010 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:04,020 : INFO : PROGRESS: at 45.39% examples, 172493 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:05,058 : INFO : PROGRESS: at 47.40% examples, 170053 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:41:06,184 : INFO : PROGRESS: at 49.72% examples, 168607 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:07,220 : INFO : PROGRESS: at 51.86% examples, 167695 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:08,265 : INFO : PROGRESS: at 54.52% examples, 168128 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:41:09,272 : INFO : PROGRESS: at 57.34% examples, 169104 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:10,358 : INFO : PROGRESS: at 60.10% examples, 170048 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:41:11,363 : INFO : PROGRESS: at 62.97% examples, 171209 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:12,414 : INFO : PROGRESS: at 65.86% examples, 171991 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:41:13,426 : INFO : PROGRESS: at 68.65% examples, 172391 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:14,444 : INFO : PROGRESS: at 71.42% examples, 173277 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:41:15,510 : INFO : PROGRESS: at 74.27% examples, 173813 words/s, in_qsize 8, out_qsize 1\n",
      "2017-06-12 17:41:16,588 : INFO : PROGRESS: at 76.66% examples, 172751 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:41:17,603 : INFO : PROGRESS: at 78.45% examples, 171583 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:41:18,606 : INFO : PROGRESS: at 80.50% examples, 171030 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:19,678 : INFO : PROGRESS: at 82.48% examples, 169739 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:20,689 : INFO : PROGRESS: at 84.37% examples, 168582 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-12 17:41:21,745 : INFO : PROGRESS: at 86.91% examples, 168372 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:22,786 : INFO : PROGRESS: at 89.93% examples, 169276 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:23,798 : INFO : PROGRESS: at 92.51% examples, 169655 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:24,827 : INFO : PROGRESS: at 95.54% examples, 170501 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:25,846 : INFO : PROGRESS: at 98.25% examples, 170960 words/s, in_qsize 7, out_qsize 0\n",
      "2017-06-12 17:41:26,405 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-06-12 17:41:26,462 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-12 17:41:26,498 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-12 17:41:26,538 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-12 17:41:26,539 : INFO : training on 9090515 raw words (7021533 effective words) took 41.0s, 171340 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7021533"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thrones2vec.train(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save to file, can be useful later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-12 17:41:26,563 : INFO : saving Word2Vec object under trained\\thrones2vec.w2v, separately None\n",
      "2017-06-12 17:41:26,572 : INFO : not storing attribute syn0norm\n",
      "2017-06-12 17:41:26,577 : INFO : not storing attribute cum_table\n",
      "2017-06-12 17:41:27,203 : INFO : saved trained\\thrones2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "thrones2vec.save(os.path.join(\"trained\", \"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-12 17:41:27,213 : INFO : loading Word2Vec object from trained\\thrones2vec.w2v\n",
      "2017-06-12 17:41:27,605 : INFO : loading wv recursively from trained\\thrones2vec.w2v.wv.* with mmap=None\n",
      "2017-06-12 17:41:27,606 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-06-12 17:41:27,608 : INFO : setting ignored attribute cum_table to None\n",
      "2017-06-12 17:41:27,611 : INFO : loaded trained\\thrones2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "thrones2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress the word vectors into 2D space and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#my video - how to visualize a dataset easily\n",
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_word_vectors_matrix = thrones2vec.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train t-SNE, this could take a minute or two...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the big picture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (word, coords[0], coords[1])\n",
    "        for word, coords in [\n",
    "            (word, all_word_vectors_matrix_2d[thrones2vec.vocab[word].index])\n",
    "            for word in thrones2vec.vocab\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"word\", \"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points.plot.scatter(\"x\", \"y\", s=10, figsize=(20, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Zoom in to some interesting places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_region(x_bounds, y_bounds):\n",
    "    slice = points[\n",
    "        (x_bounds[0] <= points.x) &\n",
    "        (points.x <= x_bounds[1]) & \n",
    "        (y_bounds[0] <= points.y) &\n",
    "        (points.y <= y_bounds[1])\n",
    "    ]\n",
    "    \n",
    "    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n",
    "    for i, point in slice.iterrows():\n",
    "        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**People related to Kingsguard ended up together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_region(x_bounds=(4.0, 4.2), y_bounds=(-0.5, -0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Food products are grouped nicely as well. Aerys (The Mad King) being close to \"roasted\" also looks sadly correct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_region(x_bounds=(0, 1), y_bounds=(4, 4.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore semantic similarities between book characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words closest to the given word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thrones2vec.most_similar(\"Stark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thrones2vec.most_similar(\"Aerys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thrones2vec.most_similar(\"direwolf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear relationships between word pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_similarity_cosmul(start1, end1, end2):\n",
    "    similarities = thrones2vec.most_similar_cosmul(\n",
    "        positive=[end2, start1],\n",
    "        negative=[end1]\n",
    "    )\n",
    "    start2 = similarities[0][0]\n",
    "    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n",
    "    return start2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearest_similarity_cosmul(\"Stark\", \"Winterfell\", \"Riverrun\")\n",
    "nearest_similarity_cosmul(\"Jaime\", \"sword\", \"wine\")\n",
    "nearest_similarity_cosmul(\"Arya\", \"Nymeria\", \"dragons\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
