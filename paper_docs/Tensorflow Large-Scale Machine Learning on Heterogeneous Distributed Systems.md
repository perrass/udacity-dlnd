# Tensorflow: Large-Scale Machine Learning on Heterogeneous Distributed Systems 

## Programming Model and Basic Concepts

A Tensorflow Computation is described by a directed graph, which is composed of a set of nodes. The graph represents a dataflow computation, with extensions for **allowing some kinds of nodes to maintain and update persistent state and for branching and looping control structures within the graph**

In a Tensorflow graph, each **node** has zero or more inputs and zero or more outputs, and represents the instantiation (实例) of an **operation**. Values that flow along **normal edges** in the graph are **tensors**, arbitrary dimensionality arrays where the underlying element type is specified or inferred at graph-construction time. Special edges, called **control dependecies**, can also exists in the graph: no data flows along such edges, but they indicate that the source node for the control dependence must finish executing before the destination node for the control dependence starts executing.

| Category                             | Examples                  |
| ------------------------------------ | ------------------------- |
| Element-wise mathematical operations | Add, Sub, ...             |
| Array Operations                     | Concat, Slice, Split, ... |
| Matrix Operations                    | MatMul, Inverse, ...      |
| Stateful Operations                  | Variable, Assign, ...     |
| Neural-net building blocks           | SoftMax, Conv2D, ...      |
| Checkpointing                        | Save, Restore, ...        |
| Queue and synchronization operations | Enqueue, Dequeue, ...     |
| Control flow operations              | Merge, Switch, Enter, ... |

#### Session

Clients programs interact with the TensorFlow system by creating a Session. There are two primary operations supported by **Session interface** are

* Extend, to augment the current graph managed by the session with additioanl nodes and edges
* Run, to execute the full graph of a few distinct subgraphs taking a set of output names that need to be computed, as well as as operational set of tensors to be fed into the graph in place of certain outputs of nodes.

#### Variables

A **variable** is a special kind of operation that returns a handle to a **persistent mutable tensor** that survives across executions of a graph, compared with most tensors not survived

## Implementation

The main components in a TensorFlow system are the **client**, which uses the Session interface to communicate with the **master**, and one or more **worker processes**, with each worker process responsible for arbitrating access to one or more computational **devices**, and executing graph nodes on those devices as instructed by the master.

#### Devices

Each worker is responsible for one or more devices, and each device has a type and a name.

### Single-Device Execution

A single worker process with a single device. The nodes of the graph are executed in an order that respects the dependencies between nodes. In particular, **we keep track of a count per node of the number of dependencies of that node that have not yet been executed**. Once this count drops to zero, the node is eligible for execution and is added to a ready queue. The **ready queue** is processed in some unspecified order, delegating execution of the kernel for a node to the device object. When a node has finished executing, the counts of all nodes that depend on the completed node are decremented.

### Multi-Device Execution

Once a system has multiple devices, there are two main complications: 

* deciding which device to place the computation for each node in the graph
* then managing the required communication of data across device boundaries implied by these placement decisions.

#### Node Placement

One imput to the placement algorithms is a **cost model**, which contains estimated of the sizes (in bytes) of the input and output tensors for each graph node, along with estimates of the computation time required for each node when presented with its input tensors. This cost model is either statically estimated based on heuristics associated with different operation types, or is measured based on an actual set of placement decisions for earlier executions of the graph.

The placement algorithm first runs a **simulated execution** of the graph. The simulation is described below and ends up **picking a device for each node in the graph using greedy heuristics**. The node to device placement generated by this simulation is also used as the placement for the real execution.

The placement algorithms starts with the sources of the computation graph, and simulates the activity on each device in the system as it progresses. For each node that is reached in this traversal, the set of feasible devices is considered (a device may not be feasible if the device does not provide a kernel that implements the particular operation). For nodes with multiple feasible devices, the placement algorithm uses a greedy heurisitc that examines the effects on the completion time of the node of placing the node on each possible device. This heurisitc takes into account that **estimated or measured execution time of the operation on that kind of device from the cost model, and also includes in order to transmit inputs to this node from other devices to the considered device**. The device where the node's operation would finish the soonest is selected as the device for that operation, and the placement process then continues onwards to make placement decisions for other nodes in the graph, including downstream nodes that are now ready for their own simulated execution.

#### Cross-Device Communication

**Once the node placement has been computed, the graph is partitioned into a set of subgraphs, one per device.** Any cross-device edge from x to y is removed and replaced by an edge from x to a new **Send** node in x's subgraph and an edge from a corresponding **Receive** node to y in y's subgraph.

![](/assets/cross_device_communication.png)

When we insert Send and Receive nodes, we **canonicalize all users of a particular tensor on a particular device to use a single Receive node**. This ensures that the **data for the needed tensor is only transmitted once** between a source device $\to$ destination device pair, and that **memory for the tensor on the destination device is only allocated once**, rathan than multiple times (e.g., see nodes b and c)

By handling communication in this manner, we also allow the scheduling of individual nodes of the graph on different devices to be **decentralized into the worker**: the Send and Receive nodes impart the necessary synchronization between different workers and devices, and the **master only needs to issue a single Run request per graph execution to each worker that has any nodes for the graph**, rather than being invovled in the scheduling of every node or every cross-device communication.

### Distributed Execution

Distributed execution of a graph is very similar to multi-device execution. After device placement, a subgraph is created per device. Send/Receive node pairs that communicate across worker processes **use remote communication mechanisms such as TCP or RDMA to move data across machine boundaries**

#### Failures

* An error in a communication between a Send and Receive node pair
* periodic health-checks from the master process to every worker process

When a failure is detected, the entire graph execution is aborted and restarted from scratch. We support consistent checkpoint-ing and recovery of this state on a restart. In particular, **each Variable node is connected to a Save node**. These Save nodes are executed periodically, say once every N iterations, or once every N seconds.When they execute, the contents of the variables are written to persistent storage, e.g., a distributed file system. **Similarly each Variable is connected to a Restore node** that is only enabled in the first iteration after a restart. 

## Extensions

### Gradient Computation

TensorFlow has built-in support for automatic gradient computation, by extending the Tensorflow graph.

![](/assets/gradient_subgraph.png)

When TensorFlow needs to compute the gradient of a tensor C with respect to some tensor I on which C depends, it first finds the path in the commputation graph from I to C. Then it backtracks from C to I, and for each operation on the backward parth it adds a node to the TensowFlow graph, composing the partial gradients along the backwards path using the chain rule. The newly added node computes the gradient function for the corresponding operation in the forward path.

Automatic gradient computation complicates optimization, particularly of memory usage. When executing "forward" computation subgraphs, a sensible heuristic breaks ties whtn deciding which ndoe to execute next by observing the order in which the graph was construceted. This generally means that temporary outputs are consumed soon after being constructed, so their memory can be reused quickly. When the heuristic is ineffective, the user can change the order of graph construction, or add control dependencies. When gradient nodes are automatically added to the graph, the user has less control, and the heuristics may break down. **In particular, tensors that are used early in a graph's execution are frequently needed again near the end of a gradient computations.  Such tensors can hold on to a lot of scarce GPU memory and unnecessarily limit the size of computations**

### Partial Execution

Two arguments to the Run call help define the exact subgraph of the computation graph that will be executed.

* Each node:port specified in inputs is replaced with a **feed** node, which will pick up the provided input tensor from specially-initialized entries in a Rendezvous object used for the Run call
* Each output name with a port is connected to a special **fetch** node that **arranges to save the output tensor and return it to the client when the Run call is complete**

![](/assets/partial_execution.png)

This figure shows an original graph on the left, and the transformed graph that results when Run is invoked with inputs=={b} and outputs=={f:0}.

### Device Constraints

Limit the device and change the placement algorithm

### Control Flow

We introduce a small set of primitive control flow operators into TensorFlow and generalize TensorFlow to handle cyclic dataflow graphs.

* The `Switch` and `Merge` operators allow us to skip the execution of an entire subgraph based on the value of a boolean tensor
* The `Enter`, `Leave` and `NextIteration` operators allow us to express interation.
* **High-level programming constructs** such as if-conditionals and while-loops **can be easily compiled into dataflow graphs with these control flow operators**

TensorFlow uses a **distributed coordination** mechanism to execute graphs with control flow. In general, a loop can contain nodes that are assigned to many different devices. Therefore, **managing the state of a loop becomes a problem of distributed termination detection** ($\color{red}{????}$). TensorFlow's solution is based on **graph rewriting**. During the graph partitioning. These ndoes implement a small state machine that orcestrates the start and termination of each iteration., and decides the termination of the loop. For each iteration, the device that owns the loop termination predicate sends a tiny control message to every participating device. The basic technique is to rewrite the graph so to memorize the values needed for the gradient computation.

### Queue

Queues are a useful feature that we have added to TensorFlow. They allow different portions of the graph to execute asynchronously, possibly at different candences, and to hand off data through Enqueue and Dequeue operations.

* Enqueue operations can block until space becomes available in the queue
* Dequeue operations can block until a desired minimum number of elements are available in the queue.

One use of queues is to allow input data to be prefectched from disk files while a previous batch of data is still being processed by the computational portion of a machine learning model

In addition to normal FIFO queues, we have also implemented a **shuffling queue**, which randomly shuffles ites elements within a large **in-memory buffer**. This shuffling functionality is useful for machine learning algorithms that want to randomize the order in which they process examples.

### Containers

A **container** is the mechanism within TensorFlow for managing longer-lived mutable state. The bakcing store for a Variable lives in a container. Using containers, it is possible to share state even across completely disjoint computation graphs associated wtih different Sessions.

