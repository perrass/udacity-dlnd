# Tensorflow: Large-Scale Machine Learning on Heterogeneous Distributed Systems 

## Programming Model and Basic Concepts

A Tensorflow Computation is described by a directed graph, which is composed of a set of nodes. The graph represents a dataflow computation, with extensions for **allowing some kinds of nodes to maintain and update persistent state and for branching and looping control structures within the graph**

In a Tensorflow graph, each **node** has zero or more inputs and zero or more outputs, and represents the instantiation (实例) of an **operation**. Values that flow along **normal edges** in the graph are **tensors**, arbitrary dimensionality arrays where the underlying element type is specified or inferred at graph-construction time. Special edges, called **control dependecies**, can also exists in the graph: no data flows along such edges, but they indicate that the source node for the control dependence must finish executing before the destination node for the control dependence starts executing.

| Category                             | Examples                  |
| ------------------------------------ | ------------------------- |
| Element-wise mathematical operations | Add, Sub, ...             |
| Array Operations                     | Concat, Slice, Split, ... |
| Matrix Operations                    | MatMul, Inverse, ...      |
| Stateful Operations                  | Variable, Assign, ...     |
| Neural-net building blocks           | SoftMax, Conv2D, ...      |
| Checkpointing                        | Save, Restore, ...        |
| Queue and synchronization operations | Enqueue, Dequeue, ...     |
| Control flow operations              | Merge, Switch, Enter, ... |

#### Session

Clients programs interact with the TensorFlow system by creating a Session. There are two primary operations supported by **Session interface** are

* Extend, to augment the current graph managed by the session with additioanl nodes and edges
* Run, to execute the full graph of a few distinct subgraphs taking a set of output names that need to be computed, as well as as operational set of tensors to be fed into the graph in place of certain outputs of nodes.

#### Variables

A **variable** is a special kind of operation that returns a handle to a **persistent mutable tensor** that survives across executions of a graph, compared with most tensors not survived

## Implementation

The main components in a TensorFlow system are the **client**, which uses the Session interface to communicate with the **master**, and one or more **worker processes**, with each worker process responsible for arbitrating access to one or more computational **devices**, and executing graph nodes on those devices as instructed by the master.

#### Devices

Each worker is responsible for one or more devices, and each device has a type and a name.

### Single-Device Execution

A single worker process with a single device. The nodes of the graph are executed in an order that respects the dependencies between nodes. In particular, **we keep track of a count per node of the number of dependencies of that node that have not yet been executed**. Once this count drops to zero, the node is eligible for execution and is added to a ready queue. The **ready queue** is processed in some unspecified order, delegating execution of the kernel for a node to the device object. When a node has finished executing, the counts of all nodes that depend on the completed node are decremented.

### Multi-Device Execution

Once a system has multiple devices, there are two main complications: 

* deciding which device to place the computation for each node in the graph
* then managing the required communication of data across device boundaries implied by these placement decisions.

#### Node Placement

One imput to the placement algorithms is a **cost model**, which contains estimated of the sizes (in bytes) of the input and output tensors for each graph node, along with estimates of the computation time required for each node when presented with its input tensors. This cost model is either statically estimated based on heuristics associated with different operation types, or is measured based on an actual set of placement decisions for earlier executions of the graph.

The placement algorithm first runs a **simulated execution** of the graph. The simulation is described below and ends up **picking a device for each node in the graph using greedy heuristics**. The node to device placement generated by this simulation is also used as the placement for the real execution.

The placement algorithms starts with the sources of the computation graph, and simulates the activity on each device in the system as it progresses. For each node that is reached in this traversal, the set of feasible devices is considered (a device may not be feasible if the device does not provide a kernel that implements the particular operation). For nodes with multiple feasible devices, the placement algorithm uses a greedy heurisitc that examines the effects on the completion time of the node of placing the node on each possible device. This heurisitc takes into account that **estimated or measured execution time of the operation on that kind of device from the cost model, and also includes in order to transmit inputs to this node from other devices to the considered device**. The device where the node's operation would finish the soonest is selected as the device for that operation, and the placement process then continues onwards to make placement decisions for other nodes in the graph, including downstream nodes that are now ready for their own simulated execution.

#### Cross-Device Communication

**Once the node placement has been computed, the graph is partitioned into a set of subgraphs, one per device.** Any cross-device edge from x to y is removed and replaced by an edge from x to a new **Send** node in x's subgraph and an edge from a corresponding **Receive** node to y in y's subgraph.

![](/assets/cross_device_communication.png)

When we insert Send and Receive nodes, we **canonicalize all users of a particular tensor on a particular device to use a single Receive node**. This ensures that the **data for the needed tensor is only transmitted once** between a source device $\to$ destination device pair, and that **memory for the tensor on the destination device is only allocated once**, rathan than multiple times (e.g., see nodes b and c)

By handling communication in this manner, we also allow the scheduling of individual nodes of the graph on different devices to be **decentralized into the worker**: the Send and Receive nodes impart the necessary synchronization between different workers and devices, and the **master only needs to issue a single Run request per graph execution to each worker that has any nodes for the graph**, rather than being invovled in the scheduling of every node or every cross-device communication.

### Distributed Execution

Distributed execution of a graph is very similar to multi-device execution. After device placement, a subgraph is created per device. Send/Receive node pairs that communicate across worker processes **use remote communication mechanisms such as TCP or RDMA to move data across machine boundaries**

#### Failures

* An error in a communication between a Send and Receive node pair
* periodic health-checks from the master process to every worker process

When a failure is detected, the entire graph execution is aborted and restarted from scratch. We support consistent checkpoint-ing and recovery of this state on a restart. In particular, **each Variable node is connected to a Save node**. These Save nodes are executed periodically, say once every N iterations, or once every N seconds.When they execute, the contents of the variables are written to persistent storage, e.g., a distributed file system. **Similarly each Variable is connected to a Restore node** that is only enabled in the first iteration after a restart. 

## Extensions

### Gradient Computation

TensorFlow has built-in support for automatic gradient computation, by extending the Tensorflow graph.

![](/assets/gradient_subgraph.png)

When TensorFlow needs to compute the gradient of a tensor C with respect to some tensor I on which C depends, it first finds the path in the commputation graph from I to C. Then it backtracks from C to I, and for each operation on the backward parth it adds a node to the TensowFlow graph, composing the partial gradients along the backwards path using the chain rule. The newly added node computes the gradient function for the corresponding operation in the forward path.

Automatic gradient computation complicates optimization, particularly of memory usage. When executing "forward" computation subgraphs, a sensible heuristic breaks ties whtn deciding which ndoe to execute next by observing the order in which the graph was construceted. This generally means that temporary outputs are consumed soon after being constructed, so their memory can be reused quickly. When the heuristic is ineffective, the user can change the order of graph construction, or add control dependencies. When gradient nodes are automatically added to the graph, the user has less control, and the heuristics may break down. **In particular, tensors that are used early in a graph's execution are frequently needed again near the end of a gradient computations.  Such tensors can hold on to a lot of scarce GPU memory and unnecessarily limit the size of computations**

### Partial Execution

Two arguments to the Run call help define the exact subgraph of the computation graph that will be executed.

* Each node:port specified in inputs is replaced with a **feed** node, which will pick up the provided input tensor from specially-initialized entries in a Rendezvous object used for the Run call
* Each output name with a port is connected to a special **fetch** node that **arranges to save the output tensor and return it to the client when the Run call is complete**

![](/assets/partial_execution.png)

This figure shows an original graph on the left, and the transformed graph that results when Run is invoked with inputs=={b} and outputs=={f:0}.

### Device Constraints

Limit the device and change the placement algorithm

### Control Flow

We introduce a small set of primitive control flow operators into TensorFlow and generalize TensorFlow to handle cyclic dataflow graphs.

* The `Switch` and `Merge` operators allow us to skip the execution of an entire subgraph based on the value of a boolean tensor
* The `Enter`, `Leave` and `NextIteration` operators allow us to express interation.
* **High-level programming constructs** such as if-conditionals and while-loops **can be easily compiled into dataflow graphs with these control flow operators**

TensorFlow uses a **distributed coordination** mechanism to execute graphs with control flow. In general, a loop can contain nodes that are assigned to many different devices. Therefore, **managing the state of a loop becomes a problem of distributed termination detection** ($\color{red}{????}$). TensorFlow's solution is based on **graph rewriting**. During the graph partitioning. These ndoes implement a small state machine that orcestrates the start and termination of each iteration., and decides the termination of the loop. For each iteration, the device that owns the loop termination predicate sends a tiny control message to every participating device. The basic technique is to rewrite the graph so to memorize the values needed for the gradient computation.

### Queue

Queues are a useful feature that we have added to TensorFlow. They allow different portions of the graph to execute asynchronously, possibly at different candences, and to hand off data through Enqueue and Dequeue operations.

* Enqueue operations can block until space becomes available in the queue
* Dequeue operations can block until a desired minimum number of elements are available in the queue.

One use of queues is to allow input data to be prefectched from disk files while a previous batch of data is still being processed by the computational portion of a machine learning model

In addition to normal FIFO queues, we have also implemented a **shuffling queue**, which randomly shuffles ites elements within a large **in-memory buffer**. This shuffling functionality is useful for machine learning algorithms that want to randomize the order in which they process examples.

### Containers

A **container** is the mechanism within TensorFlow for managing longer-lived mutable state. The bakcing store for a Variable lives in a container. Using containers, it is possible to share state even across completely disjoint computation graphs associated wtih different Sessions.

## Optimizations

### Common Subexpression Elimination

Since the construction of computation graphs is often done by many different layers of abstractions in the client code, computation graphs can easily end up with redundant copies of the same computation. To handle this, we have implemented a common subexpression pass similar to the algorithm described by **Click** that runs over the computation graph and canonicalizes multiple copies of operations with identical inputs and operation types to just a single one of these nodes, and redirects graph edges appropriately to reflect the canonicalization.

PS: 自己的理解是，不重新搭建图，而是用一个图的output的Node，做为新图的input的Node

### Controlling Data Communication and Memory Usage

TensorFlow's optimization of scheduling focuses on data transfers and memory usages. Specifically, scheduling can **reduce the time window during which intermiediate results need to be kept in memory in between operations** and hence (reduce) the peak memory consumption. Furthermore, orchestrating the communication of data across devices can **reduce contention for network resources**.

The particularly necessary and effective optimization is the **scheduling of Receive nodes for reading remote values**. If no precautions (预防) are taken, these nodes may start much earlier than necessary, possibly all at once when execution starts. By performing an as-soon-as-possible/as-late-as-possible calculation, we analyze the critical paths of graphs, in order to estimate when to start the Receive nodes. We then **insert control edges with the aim of delaying the start of these nodes until just before their results are needed**

### Asynchronous Kernels

In addition to normal synchronous kernels that complete their execution at the end the Compute method, our framework also supports **non-blocking kernels**. Such non-blocking kernels use a slightly different interface whereby the Compute method is passed a continuation that should be invoked when the kernel's execution is complete. This is an optimization for environments where **having many active threads is relatively expensive in terms of memory usage or other resources, and allows us to avoid tying up an execution thread for unbounded periods of time while waiting for I/O or other events to occur**. Examples of asynchronous kernels include the **Receive** kernel, and the **Enqueue** and **Dequeue** kernels (which might need to block if queue space is not available or if no data is available to be read, respectively)

### Optimized Libraries for Kernel Implementations

We often make use of pre-existing highly-optimized numerical libraries to implement kernels for some operations (e.g. GPU libraries for convolutional kernels for deep neural nets such as cuda-convnet and cuDNN)

### Lossy Compression

**Some machine learning algorithms, including those typically used for training neural networks, are torelant of noise and reduced precision arithmetic.** We often **use lossy compression of higher precision internal representations when sending data between devices**. For example, we often **insert special conversion nodes that convert 32-bit floating point representations into a 16-bit floating point representation, and then convert back to a 32-bit representation on the other side of the communication channel (filling in zeros)**

## Status and Experience

Critical strategies for porting the Inception model to Tensorflow:

1. Build tools to gain insight into the exact number of parameters in a given model
2. Start small and scale up
3. Always ensure that the objective (loss function) matches between machine learning systems when learning is turned off. Setting the learning rate to be zero helped us identify unexpected behavior in how we had randomly initialized variables in a model. Such an error would have been difficult to identify in a dynamic, training network
4. Make a single machine implementation match before debugging a distributed implementation
5. Guard against numerical errors. Numerical libraries are inconsistent in how they handle non-finite floating point values. Convolutional neural networks are particularly susceptible to numerical instability and will tend to diverge quite regularly during experimentation and debugging phases. Guarding against this behavior by checking for non-finite floating point values allows one to detect errors in real time as opposed to identifying divergent behavior post-hoc
6. Analyze pieces of a network and understand the magnitude of numercial error

## Common Programming Idioms

### Data Parallel Training

One simple technique for speeding up SGD is to parallelize the computation of the gradient for a mini-batch across mini-batch elements. For example, if we are using a mini-batch size of 1000 elements, we can use 10 replicas of the model to each compute the gradient for 100 elements, and then combine the gradients and apply updates to the parameters synchronously. In this case, the TensorFlow graph simply has many replicas of the portion of the graph that does the bulk f the model computation, and **a single client thread drives the entire training loop for a large graph**

![](/assets/data_parallelism.png)

This approach can also be made asynchronous. In this configuration, there is **one client thread for each of the graph replicas**

### Model Parallel Training

Model Parallel Training, where different portions of 	the model computation are done on different computational devices simultaneously for the same batch of examples, is also easy to express in TensorFlow. E.g., LSTM model used for sequence to sequence learning

![](/assets/model_parallelism.png)

### Concurrent Steps for Model Computation Pipelining

Another common way to get better utilization for training deep neural networks is to pipeline the computation of the model within the same devices, by running  a small number of concurent steps within the same set of devices

![](/assets/concurrent_parallelism.png)